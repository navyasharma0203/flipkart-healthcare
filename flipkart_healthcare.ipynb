{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation for Segmentation"
      ],
      "metadata": {
        "id": "QsPacVAGuC7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Images"
      ],
      "metadata": {
        "id": "hC5gycGauIH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRk8PuyZs9yP"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (if your dataset is stored on Google Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "G6qc0B-QtkQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the train and test image directories\n",
        "train_image_dir = '/content/drive/MyDrive/YourDatasetFolder/train'\n",
        "test_image_dir = '/content/drive/MyDrive/YourDatasetFolder/test'"
      ],
      "metadata": {
        "id": "mB2mDDbatmqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images from a directory\n",
        "def load_images_from_directory(directory_path):\n",
        "    images = []\n",
        "    for filename in tqdm(os.listdir(directory_path)):\n",
        "        if filename.endswith('.jpg'):\n",
        "            image_path = os.path.join(directory_path, filename)\n",
        "            image = cv2.imread(image_path)  # Load image using OpenCV\n",
        "            images.append(image)\n",
        "    return images"
      ],
      "metadata": {
        "id": "gd_Ci0iztpW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test images\n",
        "train_images = load_images_from_directory(train_image_dir)\n",
        "test_images = load_images_from_directory(test_image_dir)"
      ],
      "metadata": {
        "id": "qCo_Tkf1ttH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image lists to numpy arrays\n",
        "train_images_array = np.array(train_images)\n",
        "test_images_array = np.array(test_images)"
      ],
      "metadata": {
        "id": "rlfF4-RFtwKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of loaded images\n",
        "print(f\"Number of train images: {len(train_images)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")"
      ],
      "metadata": {
        "id": "nMPdXBUWt7zC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load Groundtruth Masks"
      ],
      "metadata": {
        "id": "VuFdKSVeuNkr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess masks from a directory\n",
        "def load_and_preprocess_masks(directory_path):\n",
        "    masks = []\n",
        "    for filename in tqdm(os.listdir(directory_path)):\n",
        "        if filename.endswith('.tif'):\n",
        "            mask_path = os.path.join(directory_path, filename)\n",
        "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load mask using OpenCV\n",
        "            mask = cv2.threshold(mask, 127, 255, cv2.THRESH_BINARY)[1]  # Thresholding\n",
        "            masks.append(mask)\n",
        "    return masks"
      ],
      "metadata": {
        "id": "axH_X0bZuShY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test masks\n",
        "train_masks = load_and_preprocess_masks(train_mask_dir)\n",
        "test_masks = load_and_preprocess_masks(test_mask_dir)"
      ],
      "metadata": {
        "id": "dEzUjpZ4u66R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert mask lists to numpy arrays\n",
        "train_masks_array = np.array(train_masks)\n",
        "test_masks_array = np.array(test_masks)"
      ],
      "metadata": {
        "id": "qxAv8EQsu702"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of loaded images and masks\n",
        "print(f\"Number of train images: {len(train_images)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")\n",
        "print(f\"Number of train masks: {len(train_masks)}\")\n",
        "print(f\"Number of test masks: {len(test_masks)}\")"
      ],
      "metadata": {
        "id": "GrNPSJjau-9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess Images and Masks"
      ],
      "metadata": {
        "id": "IExu4ycQvP7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target image dimensions\n",
        "target_height = 256\n",
        "target_width = 256"
      ],
      "metadata": {
        "id": "KFhTMGClvVxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess images and masks\n",
        "def preprocess_images_masks(images, masks):\n",
        "    preprocessed_images = []\n",
        "    preprocessed_masks = []\n",
        "\n",
        "    for image, mask in tqdm(zip(images, masks)):\n",
        "        # Resize image and mask to the target dimensions\n",
        "        resized_image = cv2.resize(image, (target_width, target_height))\n",
        "        resized_mask = cv2.resize(mask, (target_width, target_height), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Normalize image pixel values to [0, 1]\n",
        "        normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Append preprocessed images and masks\n",
        "        preprocessed_images.append(normalized_image)\n",
        "        preprocessed_masks.append(resized_mask)\n",
        "\n",
        "    return preprocessed_images, preprocessed_masks"
      ],
      "metadata": {
        "id": "EeVPNF3LvWin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess train and test images and masks\n",
        "train_preprocessed_images, train_preprocessed_masks = preprocess_images_masks(train_images_array, train_masks_array)\n",
        "test_preprocessed_images, test_preprocessed_masks = preprocess_images_masks(test_images_array, test_masks_array)"
      ],
      "metadata": {
        "id": "NF7JujG4vdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert preprocessed image and mask lists to numpy arrays\n",
        "train_preprocessed_images_array = np.array(train_preprocessed_images)\n",
        "train_preprocessed_masks_array = np.array(train_preprocessed_masks)\n",
        "test_preprocessed_images_array = np.array(test_preprocessed_images)\n",
        "test_preprocessed_masks_array = np.array(test_preprocessed_masks)"
      ],
      "metadata": {
        "id": "RIcvAM1Rvkn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of preprocessed images and masks\n",
        "print(f\"Shape of preprocessed train images: {train_preprocessed_images_array.shape}\")\n",
        "print(f\"Shape of preprocessed train masks: {train_preprocessed_masks_array.shape}\")\n",
        "print(f\"Shape of preprocessed test images: {test_preprocessed_images_array.shape}\")\n",
        "print(f\"Shape of preprocessed test masks: {test_preprocessed_masks_array.shape}\")"
      ],
      "metadata": {
        "id": "G24oEs3pvnIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation for Disease Grading"
      ],
      "metadata": {
        "id": "1TTx65bGwCjb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Image Dataset"
      ],
      "metadata": {
        "id": "0IgdBVy3wF9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the train and test image directories (same as before)\n",
        "train_image_dir = '/content/drive/MyDrive/YourDatasetFolder/train'\n",
        "test_image_dir = '/content/drive/MyDrive/YourDatasetFolder/test'"
      ],
      "metadata": {
        "id": "AJsXgtq_wH3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images from a directory\n",
        "def load_images_from_directory(directory_path):\n",
        "    images = []\n",
        "    for filename in tqdm(os.listdir(directory_path)):\n",
        "        if filename.endswith('.jpg'):\n",
        "            image_path = os.path.join(directory_path, filename)\n",
        "            image = cv2.imread(image_path)  # Load image using OpenCV\n",
        "            images.append(image)\n",
        "    return images"
      ],
      "metadata": {
        "id": "CMug29MDwNV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test images\n",
        "train_images = load_images_from_directory(train_image_dir)\n",
        "test_images = load_images_from_directory(test_image_dir)"
      ],
      "metadata": {
        "id": "0KksgJq-wQff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert image lists to numpy arrays\n",
        "train_images_array = np.array(train_images)\n",
        "test_images_array = np.array(test_images)"
      ],
      "metadata": {
        "id": "wrFdX4W-wTH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of loaded images\n",
        "print(f\"Number of train images: {len(train_images)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")"
      ],
      "metadata": {
        "id": "bh3jh9LkwVlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load Groundtruth Labels"
      ],
      "metadata": {
        "id": "NQ4Xdm2ywYlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to the CSV file containing groundtruth labels (replace with your actual path)\n",
        "labels_csv_path = '/content/drive/MyDrive/YourDatasetFolder/disease_grading_labels.csv'"
      ],
      "metadata": {
        "id": "wFqvGFywwcSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load groundtruth labels from CSV file using Pandas\n",
        "def load_labels_from_csv(csv_path):\n",
        "    labels_data = pd.read_csv(csv_path)\n",
        "    return labels_data"
      ],
      "metadata": {
        "id": "RDfHPRsswsVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load groundtruth labels from CSV\n",
        "labels_data = load_labels_from_csv(labels_csv_path)\n",
        "\n",
        "# Display the first few rows of the labels DataFrame\n",
        "print(labels_data.head())"
      ],
      "metadata": {
        "id": "2Y-jYguHwvCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess Images"
      ],
      "metadata": {
        "id": "wUfD6_P0wxdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target image dimensions\n",
        "target_height = 256\n",
        "target_width = 256"
      ],
      "metadata": {
        "id": "u3c9XERkwzod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess images\n",
        "def preprocess_images(images):\n",
        "    preprocessed_images = []\n",
        "\n",
        "    for image in tqdm(images):\n",
        "        # Resize image to the target dimensions\n",
        "        resized_image = cv2.resize(image, (target_width, target_height))\n",
        "\n",
        "        # Normalize image pixel values to [0, 1]\n",
        "        normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Append preprocessed images\n",
        "        preprocessed_images.append(normalized_image)\n",
        "\n",
        "    return preprocessed_images"
      ],
      "metadata": {
        "id": "y_PPTUqyw-rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess train and test images\n",
        "train_preprocessed_images = preprocess_images(train_images_array)\n",
        "test_preprocessed_images = preprocess_images(test_images_array)\n",
        "\n",
        "# Convert preprocessed image lists to numpy arrays\n",
        "train_preprocessed_images_array = np.array(train_preprocessed_images)\n",
        "test_preprocessed_images_array = np.array(test_preprocessed_images)\n"
      ],
      "metadata": {
        "id": "Z6d46597xCUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of preprocessed images\n",
        "print(f\"Shape of preprocessed train images: {train_preprocessed_images_array.shape}\")\n",
        "print(f\"Shape of preprocessed test images: {test_preprocessed_images_array.shape}\")"
      ],
      "metadata": {
        "id": "6BfTsAdGxFk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation for Localization"
      ],
      "metadata": {
        "id": "Hdk6zvSvxJDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load Images"
      ],
      "metadata": {
        "id": "sBWLxe9fxNAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the train and test image directories (same as before)\n",
        "train_image_dir = '/content/drive/MyDrive/YourDatasetFolder/train'\n",
        "test_image_dir = '/content/drive/MyDrive/YourDatasetFolder/test'"
      ],
      "metadata": {
        "id": "y_NemFmyxOjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load images from a directory\n",
        "def load_images_from_directory(directory_path):\n",
        "    images = []\n",
        "    for filename in tqdm(os.listdir(directory_path)):\n",
        "        if filename.endswith('.jpg'):\n",
        "            image_path = os.path.join(directory_path, filename)\n",
        "            image = cv2.imread(image_path)  # Load image using OpenCV\n",
        "            images.append(image)\n",
        "    return images"
      ],
      "metadata": {
        "id": "84Jl-orkxWEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load train and test images\n",
        "train_images = load_images_from_directory(train_image_dir)\n",
        "test_images = load_images_from_directory(test_image_dir)\n",
        "\n",
        "# Convert image lists to numpy arrays\n",
        "train_images_array = np.array(train_images)\n",
        "test_images_array = np.array(test_images)"
      ],
      "metadata": {
        "id": "2t0paGpUxYt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of loaded images\n",
        "print(f\"Number of train images: {len(train_images)}\")\n",
        "print(f\"Number of test images: {len(test_images)}\")"
      ],
      "metadata": {
        "id": "_kzwDSqzxbTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load groundtruth Labels"
      ],
      "metadata": {
        "id": "SSXpJDWNxdyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths to the CSV files containing groundtruth labels (replace with your actual paths)\n",
        "optic_disc_csv_path = '/content/drive/MyDrive/YourDatasetFolder/optic_disc_labels.csv'\n",
        "fovea_csv_path = '/content/drive/MyDrive/YourDatasetFolder/fovea_labels.csv'"
      ],
      "metadata": {
        "id": "S902axY_xgK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load groundtruth labels from CSV files using Pandas\n",
        "def load_labels_from_csv(csv_path):\n",
        "    labels_data = pd.read_csv(csv_path)\n",
        "    return labels_data"
      ],
      "metadata": {
        "id": "gKgkroldxobS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load groundtruth labels for optic disc and fovea\n",
        "optic_disc_labels_data = load_labels_from_csv(optic_disc_csv_path)\n",
        "fovea_labels_data = load_labels_from_csv(fovea_csv_path)"
      ],
      "metadata": {
        "id": "hY6x8N2NxrXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the labels DataFrames\n",
        "print(\"Optic Disc Labels:\")\n",
        "print(optic_disc_labels_data.head())\n",
        "\n",
        "print(\"\\nFovea Labels:\")\n",
        "print(fovea_labels_data.head())"
      ],
      "metadata": {
        "id": "FyMSF8evxtrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Preprocess Images"
      ],
      "metadata": {
        "id": "5p0omoLex1t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define target image dimensions\n",
        "target_height = 256\n",
        "target_width = 256"
      ],
      "metadata": {
        "id": "NC5JpdFdx3fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess images\n",
        "def preprocess_images(images):\n",
        "    preprocessed_images = []\n",
        "\n",
        "    for image in tqdm(images):\n",
        "        # Resize image to the target dimensions\n",
        "        resized_image = cv2.resize(image, (target_width, target_height))\n",
        "\n",
        "        # Normalize image pixel values to [0, 1]\n",
        "        normalized_image = resized_image.astype(np.float32) / 255.0\n",
        "\n",
        "        # Append preprocessed images\n",
        "        preprocessed_images.append(normalized_image)\n",
        "\n",
        "    return preprocessed_images"
      ],
      "metadata": {
        "id": "kRIXD9Qcx6Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess train and test images\n",
        "train_preprocessed_images = preprocess_images(train_images_array)\n",
        "test_preprocessed_images = preprocess_images(test_images_array)\n",
        "\n",
        "# Convert preprocessed image lists to numpy arrays\n",
        "train_preprocessed_images_array = np.array(train_preprocessed_images)\n",
        "test_preprocessed_images_array = np.array(test_preprocessed_images)"
      ],
      "metadata": {
        "id": "tDBqs-9Mx-gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the shape of preprocessed images\n",
        "print(f\"Shape of preprocessed train images: {train_preprocessed_images_array.shape}\")\n",
        "print(f\"Shape of preprocessed test images: {test_preprocessed_images_array.shape}\")"
      ],
      "metadata": {
        "id": "hf5_Ai4nyBDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Split"
      ],
      "metadata": {
        "id": "8FqeCEy8yi4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train set into training and validation subsets\n",
        "train_images, val_images, train_labels, val_labels = train_test_split(\n",
        "    train_preprocessed_images_array, train_labels_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create TensorFlow Dataset objects for training and validation\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "\n",
        "# Shuffle, batch, and prefetch the datasets\n",
        "train_dataset = train_dataset.shuffle(len(train_images)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Display the number of batches in each dataset\n",
        "print(f\"Number of batches in train dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of batches in validation dataset: {len(val_dataset)}\")\n"
      ],
      "metadata": {
        "id": "WZEdGXdBykRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading Pipelining"
      ],
      "metadata": {
        "id": "PkCry1BVy963"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess images and labels\n",
        "def preprocess_image(image, label):\n",
        "    # Apply any additional preprocessing to the images\n",
        "    # For example: image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # Normalize image pixel values to [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Resize images (example: resize to 224x224)\n",
        "    image = tf.image.resize(image, (224, 224))\n",
        "\n",
        "    # Apply data augmentation (example: random flip)\n",
        "    if tf.random.uniform(()) > 0.5:\n",
        "        image = tf.image.flip_left_right(image)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "Ec3TUdH6zmFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing and configure the datasets\n",
        "train_dataset = train_dataset.map(preprocess_image)\n",
        "val_dataset = val_dataset.map(preprocess_image)\n",
        "\n",
        "# Shuffle, batch, and prefetch the datasets\n",
        "train_dataset = train_dataset.shuffle(len(train_images)).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Display the number of batches in each dataset\n",
        "print(f\"Number of batches in train dataset: {len(train_dataset)}\")\n",
        "print(f\"Number of batches in validation dataset: {len(val_dataset)}\")"
      ],
      "metadata": {
        "id": "n_xwPQ2uzATj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN Architecture"
      ],
      "metadata": {
        "id": "X9tKp2R9AGQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the datasets into training and validation subsets\n",
        "train_images_seg, val_images_seg, train_masks, val_masks = train_test_split(\n",
        "    train_preprocessed_images_seg_array, train_masks_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_images_loc, val_images_loc, train_optic_disc_labels, train_fovea_labels = train_test_split(\n",
        "    train_preprocessed_images_loc_array, train_optic_disc_labels_array, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_images_grade, val_images_grade, train_grade_labels, val_grade_labels = train_test_split(\n",
        "    train_preprocessed_images_grade_array, train_grade_labels_array, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "SHU5IDmgAIn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing masks/labels"
      ],
      "metadata": {
        "id": "CdZu2Xk0BWGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess images and masks/labels\n",
        "def preprocess_image(image, mask_or_label):\n",
        "    image = image / 255.0\n",
        "    return image, mask_or_label"
      ],
      "metadata": {
        "id": "WdijKWuvANzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring Datasets"
      ],
      "metadata": {
        "id": "QvMronqUBYsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing and configure the datasets\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_seg = tf.data.Dataset.from_tensor_slices((train_images_seg, train_masks))\n",
        "val_dataset_seg = tf.data.Dataset.from_tensor_slices((val_images_seg, val_masks))\n",
        "\n",
        "train_dataset_loc = tf.data.Dataset.from_tensor_slices((train_images_loc, train_optic_disc_labels))\n",
        "val_dataset_loc = tf.data.Dataset.from_tensor_slices((val_images_loc, val_optic_disc_labels))\n",
        "\n",
        "train_dataset_grade = tf.data.Dataset.from_tensor_slices((train_images_grade, train_grade_labels))\n",
        "val_dataset_grade = tf.data.Dataset.from_tensor_slices((val_images_grade, val_grade_labels))\n",
        "\n",
        "train_dataset_seg = train_dataset_seg.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_seg = val_dataset_seg.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset_loc = train_dataset_loc.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_loc = val_dataset_loc.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset_grade = train_dataset_grade.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_grade = val_dataset_grade.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "PsF36cY4AOnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complinging model for each task"
      ],
      "metadata": {
        "id": "5clmuf1XBb2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define and compile a model for each task\n",
        "\n",
        "# Segmentation Model\n",
        "seg_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Assuming binary segmentation\n",
        "])\n",
        "\n",
        "seg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "W3X_-rWEAa19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Localization Model\n",
        "loc_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(2, activation='linear')  # Assuming 2D localization coordinates\n",
        "])\n",
        "\n",
        "loc_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n"
      ],
      "metadata": {
        "id": "IHL9juPKAdj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Disease Grading Model\n",
        "grade_model = Sequential([\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)),\n",
        "    Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(5, activation='softmax')  # Assuming 5 classes for disease grading\n",
        "])\n",
        "\n",
        "grade_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "oLumGobyAj41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the models"
      ],
      "metadata": {
        "id": "QSd50rBjBgkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the models\n",
        "\n",
        "# Train Segmentation Model\n",
        "seg_history = seg_model.fit(\n",
        "    train_dataset_seg,\n",
        "    epochs=20,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_seg\n",
        ")\n",
        "\n",
        "# Train Localization Model\n",
        "loc_history = loc_model.fit(\n",
        "    train_dataset_loc,\n",
        "    epochs=20,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_loc\n",
        ")\n",
        "\n",
        "# Train Disease Grading Model\n",
        "grade_history = grade_model.fit(\n",
        "    train_dataset_grade,\n",
        "    epochs=20,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_grade\n",
        ")\n"
      ],
      "metadata": {
        "id": "mm4Ma0uZAsFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the models"
      ],
      "metadata": {
        "id": "WP2ChVJCBrHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg_test_loss, seg_test_acc = seg_model.evaluate(val_images_seg, val_masks)\n",
        "print(f\"Segmentation Model Test accuracy: {seg_test_acc}\")\n",
        "\n",
        "loc_test_loss, loc_test_mae = loc_model.evaluate(val_images_loc, val_optic_disc_labels)\n",
        "print(f\"Localization Model Test MAE: {loc_test_mae}\")\n",
        "\n",
        "grade_test_loss, grade_test_acc = grade_model.evaluate(val_images_grade, val_grade_labels)\n",
        "print(f\"Disease Grading Model Test accuracy: {grade_test_acc}\")"
      ],
      "metadata": {
        "id": "oVQyWT_vBuBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning the models"
      ],
      "metadata": {
        "id": "JRvsiRaOBrAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune Segmentation Model\n",
        "# You can adjust learning rates, optimizers, and other hyperparameters\n",
        "seg_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "seg_history = seg_model.fit(\n",
        "    train_dataset_seg,\n",
        "    epochs=10,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_seg\n",
        ")\n",
        "\n",
        "# Fine-tune Localization Model\n",
        "# You can adjust learning rates, optimizers, and other hyperparameters\n",
        "loc_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "loc_history = loc_model.fit(\n",
        "    train_dataset_loc,\n",
        "    epochs=10,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_loc\n",
        ")\n",
        "\n",
        "# Fine-tune Disease Grading Model\n",
        "# You can adjust learning rates, optimizers, and other hyperparameters\n",
        "grade_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "grade_history = grade_model.fit(\n",
        "    train_dataset_grade,\n",
        "    epochs=10,  # Adjust the number of epochs based on your needs\n",
        "    validation_data=val_dataset_grade\n",
        ")"
      ],
      "metadata": {
        "id": "u7t8-z33B-yO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparamter Tuning"
      ],
      "metadata": {
        "id": "Ddhnev-fCFvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to build the Segmentation Model for hyperparameter tuning\n",
        "def build_seg_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(hp.Int('conv1_units', min_value=32, max_value=128, step=32), (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\n",
        "    model.add(Conv2D(hp.Int('conv2_units', min_value=32, max_value=128, step=32), (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv3_units', min_value=64, max_value=256, step=64), (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(hp.Int('conv4_units', min_value=64, max_value=256, step=64), (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Conv2D(hp.Int('conv5_units', min_value=128, max_value=512, step=128), (3, 3), activation='relu', padding='same'))\n",
        "    model.add(Conv2D(hp.Int('conv6_units', min_value=128, max_value=512, step=128), (3, 3), activation='relu', padding='same'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(hp.Float('dropout_rate', min_value=0.3, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Assuming binary segmentation\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "oc9D4ShvCJCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning for Segmentation Model\n",
        "seg_tuner = RandomSearch(\n",
        "    build_seg_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,  # Adjust based on your resources\n",
        "    directory='tuning_dir',  # Directory to store tuning results\n",
        "    project_name='seg_tuning'\n",
        ")\n",
        "\n",
        "seg_tuner.search(train_dataset_seg, epochs=10, validation_data=val_dataset_seg)"
      ],
      "metadata": {
        "id": "v7jc1ZhBCob5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the best hyperparameters and build the final Segmentation Model\n",
        "best_seg_hp = seg_tuner.get_best_hyperparameters()[0]\n",
        "final_seg_model = build_seg_model(best_seg_hp)\n",
        "\n",
        "# Train the final Segmentation Model with the best hyperparameters\n",
        "final_seg_model.fit(train_dataset_seg, epochs=20, validation_data=val_dataset_seg)\n",
        "\n",
        "# Perform similar steps for hyperparameter tuning and training for Localization and Disease Grading Models\n",
        "# You'll need to define build_loc_model() and build_grade_model() functions similar to build_seg_model()\n",
        "\n",
        "# Save the trained models for future use\n",
        "final_seg_model.save(\"final_seg_model.h5\")\n",
        "final_loc_model.save(\"final_loc_model.h5\")\n",
        "final_grade_model.save(\"final_grade_model.h5\")"
      ],
      "metadata": {
        "id": "lAeUgT6MCyRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Quantization"
      ],
      "metadata": {
        "id": "HUyWXdjtDWOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained Segmentation Model\n",
        "seg_model = load_model('final_seg_model.h5')\n",
        "\n",
        "# Convert the Segmentation Model to a quantized format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(seg_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "\n",
        "# Save the quantized model\n",
        "with open('quantized_seg_model.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)"
      ],
      "metadata": {
        "id": "6MXhY27PDYaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Pruning"
      ],
      "metadata": {
        "id": "jggzsD6tEVrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained Localization Model\n",
        "loc_model = load_model('final_loc_model.h5')\n",
        "\n",
        "# Define a function to preprocess images and labels for the model\n",
        "def preprocess_image(image, label):\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Apply preprocessing and configure the datasets\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_loc = train_dataset_loc.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_loc = val_dataset_loc.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Apply pruning to the Localization Model\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.80, begin_step=0, end_step=1000)}\n",
        "pruned_model = prune_low_magnitude(loc_model, **pruning_params)\n",
        "\n",
        "# Retain the original optimizer for fine-tuning\n",
        "pruned_model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Fine-tune the pruned model to recover accuracy\n",
        "pruned_model.fit(train_dataset_loc, epochs=10, validation_data=val_dataset_loc)\n",
        "\n",
        "# Save the pruned model\n",
        "pruned_model.save('pruned_loc_model.h5')"
      ],
      "metadata": {
        "id": "ER5tOVMnEXPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model acceleration"
      ],
      "metadata": {
        "id": "J8yBrM9pFKeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your quantized model\n",
        "interpreter = tf.lite.Interpreter(model_path='quantized_seg_model.tflite')\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Apply preprocessing and configure the datasets\n",
        "batch_size = 32\n",
        "\n",
        "test_dataset_seg = test_dataset_seg.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Inference using GPU acceleration\n",
        "total_inference_time = 0\n",
        "num_inferences = 0\n",
        "\n",
        "for images, _ in test_dataset_seg:\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Run inference\n",
        "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "    interpreter.invoke()\n",
        "    output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_inference_time += end_time - start_time\n",
        "    num_inferences += images.shape[0]\n",
        "\n",
        "average_inference_time = total_inference_time / num_inferences\n",
        "print(f'Average Inference Time per Image: {average_inference_time:.4f} seconds')"
      ],
      "metadata": {
        "id": "OOacVwldFMem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knowledge Distillation"
      ],
      "metadata": {
        "id": "sDOosXnDFa-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your teacher and student models\n",
        "teacher_model = load_model('final_teacher_model.h5')\n",
        "student_model = load_model('final_student_model.h5')\n",
        "\n",
        "# Define a function to preprocess images and labels for the models\n",
        "def preprocess_image(image, label):\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Assuming you have loaded and preprocessed the datasets for knowledge distillation\n",
        "\n",
        "# Continue with loading, preprocessing, and splitting the datasets\n",
        "\n",
        "# Apply preprocessing and configure the datasets\n",
        "batch_size = 32\n",
        "\n",
        "train_dataset_teacher = train_dataset_teacher.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "train_dataset_student = train_dataset_student.map(preprocess_image).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Define a custom loss function for knowledge distillation\n",
        "def distillation_loss(y_true, y_pred):\n",
        "    alpha = 0.1  # Temperature parameter\n",
        "    return alpha * MeanSquaredError()(y_true, y_pred)\n",
        "\n",
        "# Compile the student model with distillation loss\n",
        "student_model.compile(optimizer=Adam(learning_rate=0.001), loss=distillation_loss, metrics=['accuracy'])\n",
        "\n",
        "# Perform knowledge distillation\n",
        "student_model.fit(\n",
        "    train_dataset_student,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset_student,\n",
        "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "id": "LI7FBRBvFc81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural Architectural Search (NAS)"
      ],
      "metadata": {
        "id": "9LlKUOd4GeSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the search space for architecture\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(hp.Int('units_1', min_value=32, max_value=256, step=32), (3, 3), activation='relu', input_shape=(256, 256, 3)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(2, 2))\n",
        "    # Add more layers based on hyperparameters\n",
        "    # ...\n",
        "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# Define the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='tuner_results',\n",
        "    project_name='segmentation_tuner'\n",
        ")\n",
        "\n",
        "# Search for the best architecture\n",
        "tuner.search(train_dataset_seg, epochs=10, validation_data=val_dataset_seg)\n",
        "\n",
        "# Get the best model\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n"
      ],
      "metadata": {
        "id": "0Xy9og9UGht0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efficient Model Design"
      ],
      "metadata": {
        "id": "TGIuiVuYGr7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = MobileNetV2(input_shape=(256, 256, 3), include_top=False, weights='imagenet')\n",
        "base_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "8rGCDvHaGvt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reduce Input Size"
      ],
      "metadata": {
        "id": "Wl3MW9aFHH6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resized_train_dataset_seg = train_dataset_seg.map(lambda x, y: (tf.image.resize(x, (128, 128)), y))\n",
        "resized_val_dataset_seg = val_dataset_seg.map(lambda x, y: (tf.image.resize(x, (128, 128)), y))"
      ],
      "metadata": {
        "id": "h4rLaonXHLEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Size Optimization"
      ],
      "metadata": {
        "id": "zLRBZkRPHNhw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Try different values\n",
        "train_dataset_seg = train_dataset_seg.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_dataset_seg = val_dataset_seg.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "iY6lOAlLHQ2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuous Learning"
      ],
      "metadata": {
        "id": "jx2_jZgJJngI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your original trained Segmentation Model\n",
        "original_model = load_model('final_seg_model.h5')\n",
        "\n",
        "# Load new data for continuous learning\n",
        "# Assuming you have the new dataset stored in a directory named 'new_data'\n",
        "new_data_dir = 'new_data'\n",
        "\n",
        "# Create ImageDataGenerator for new dataset\n",
        "new_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Load and preprocess the new training dataset\n",
        "new_train_dataset = new_datagen.flow_from_directory(\n",
        "    directory=os.path.join(new_data_dir, 'train'),\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Load and preprocess the new validation dataset\n",
        "new_val_dataset = new_datagen.flow_from_directory(\n",
        "    directory=os.path.join(new_data_dir, 'val'),\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Fine-tune the original model with new data\n",
        "def fine_tune_model(model, new_train_data, new_val_data, epochs=5):\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(new_train_data, epochs=epochs, validation_data=new_val_data)\n",
        "    return model\n",
        "\n",
        "# Fine-tune the original model with new data\n",
        "fine_tuned_model = fine_tune_model(original_model, new_train_dataset, new_val_dataset)\n",
        "\n",
        "# Save the fine-tuned model\n",
        "fine_tuned_model.save('fine_tuned_seg_model.h5')\n"
      ],
      "metadata": {
        "id": "6A-juHIMJprb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation"
      ],
      "metadata": {
        "id": "EMA7oZo0LMxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained model\n",
        "model = tf.keras.models.load_model('trained_model.h5')\n",
        "\n",
        "# Assuming you have loaded and preprocessed the evaluation dataset\n",
        "eval_data_dir = 'path_to_evaluation_data'  # Replace with the actual path\n",
        "\n",
        "# Create an ImageDataGenerator for evaluation data\n",
        "eval_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1./255  # Normalize pixel values\n",
        ")\n",
        "\n",
        "# Load and preprocess the evaluation dataset\n",
        "eval_dataset = eval_datagen.flow_from_directory(\n",
        "    directory=eval_data_dir,\n",
        "    target_size=(256, 256),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',  # Change based on your problem (e.g., 'binary', 'none' for regression)\n",
        "    shuffle=False  # Important: Do not shuffle evaluation data\n",
        ")\n",
        "\n",
        "# Evaluate the model on the evaluation dataset\n",
        "eval_loss, eval_accuracy = model.evaluate(eval_dataset)\n",
        "\n",
        "print(f'Evaluation Loss: {eval_loss:.4f}')\n",
        "print(f'Evaluation Accuracy: {eval_accuracy:.4f}')\n"
      ],
      "metadata": {
        "id": "Y7wU7SqwLOP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the evaluation dataset\n",
        "eval_predictions = model.predict(eval_dataset)\n",
        "eval_labels = eval_dataset.labels\n",
        "\n",
        "# Convert predictions to class labels\n",
        "eval_pred_labels = tf.argmax(eval_predictions, axis=1)\n",
        "\n",
        "# Generate classification report\n",
        "class_names = eval_dataset.class_indices\n",
        "class_names = dict((v, k) for k, v in class_names.items())\n",
        "report = classification_report(eval_labels, eval_pred_labels, target_names=class_names.values())\n",
        "\n",
        "# Calculate confusion matrix\n",
        "conf_matrix = confusion_matrix(eval_labels, eval_pred_labels)\n",
        "\n",
        "print(f'Accuracy: {accuracy:.4f}')\n",
        "print(report)\n",
        "print('Confusion Matrix:')\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "m5TUqw-eLy3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Interpretation"
      ],
      "metadata": {
        "id": "KLAonD1cMST6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names.values(), yticklabels=class_names.values())\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XCwTRy4XMUBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}